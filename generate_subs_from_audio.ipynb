{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMVI67BAvaKW/DNxcv+J2wm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cec95be4d1e944f99beb1205a9eeff1c": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_ad693ff2524f406d9cc5d026a0b138d6",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "Processing 30s chunks one by one \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[33m0:02:22\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Processing 30s chunks one by one <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:02:22</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "ad693ff2524f406d9cc5d026a0b138d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aniketjivani/generative_experiments/blob/master/generate_subs_from_audio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCbO6UJvHMIi",
        "outputId": "b0ca904a-9b27-481d-89a5-aadf20252b3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai-whisper\n",
            "  Downloading openai-whisper-20240930.tar.gz (800 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/800.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/800.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m798.7/800.5 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (1.26.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (4.66.6)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (10.5.0)\n",
            "Collecting tiktoken (from openai-whisper)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting triton>=2.0.0 (from openai-whisper)\n",
            "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2024.8.30)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803320 sha256=642bd691f62eb21f374d8925be1d145e3317531900ac2f6e63918694d633c197\n",
            "  Stored in directory: /root/.cache/pip/wheels/dd/4a/1f/d1c4bf3b9133c8168fe617ed979cab7b14fe381d059ffb9d83\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: pydub, triton, tiktoken, openai-whisper\n",
            "Successfully installed openai-whisper-20240930 pydub-0.25.1 tiktoken-0.8.0 triton-3.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install openai-whisper pydub torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6abcqJ-HZFO",
        "outputId": "121f3f42-c6ea-4c10-e2f4-d500e207db8c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Python code to convert video to audio\n",
        "import moviepy.editor as mp\n",
        "from datetime import datetime, timedelta\n",
        "from pydub import AudioSegment\n",
        "import os\n",
        "from rich.progress import track\n",
        "# Insert local audio file path\n",
        "# audioclip = mp.AudioFileClip(r\"/content/drive/MyDrive/audio_transcribe/audio_sciml_lecture.m4a\")\n",
        "\n",
        "audioclip = AudioSegment.from_file(\"/content/drive/MyDrive/audio_transcribe/audio_sciml_lecture.m4a\")\n",
        "\n",
        "shortclip = audioclip[:10000]\n",
        "\n",
        "# clip for the first three minutes.\n",
        "longclip = audioclip[:180000]\n",
        "\n",
        "# # Insert Local Video File Path\n",
        "# clip = mp.VideoFileClip(r\"Video File\")\n",
        "\n",
        "# # Insert Local Audio File Path\n",
        "# clip.audio.write_audiofile(r\"Audio File\")"
      ],
      "metadata": {
        "id": "WxjidMReIsR_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # play audioclip in Colab\n",
        "# from IPython.display import Audio\n",
        "# Audio(shortclip, rate=22050)"
      ],
      "metadata": {
        "id": "pdgpqICYPA4b"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to split large audio into chunks\n",
        "def split_audio(input_audio_path, chunk_length_ms=30000):\n",
        "    audio = AudioSegment.from_file(input_audio_path)\n",
        "    duration_ms = len(audio)\n",
        "    chunks = []\n",
        "\n",
        "    for start_ms in range(0, duration_ms, chunk_length_ms):\n",
        "        end_ms = min(start_ms + chunk_length_ms, duration_ms)\n",
        "        chunk = audio[start_ms:end_ms]\n",
        "        chunks.append(chunk)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Function to transcribe and translate audio using Whisper\n",
        "def transcribe_and_translate(audio_chunk, model):\n",
        "    # Save the chunk as a temporary file\n",
        "    temp_file = \"/content/drive/MyDrive/audio_transcribe/temp_chunk.wav\"\n",
        "    audio_chunk.export(temp_file, format=\"wav\")\n",
        "\n",
        "    # Transcribe and translate to English with timestamps\n",
        "    result = model.transcribe(temp_file, task=\"translate\")\n",
        "\n",
        "    # Remove temporary file\n",
        "    os.remove(temp_file)\n",
        "\n",
        "    return result\n",
        "\n",
        "# Function to convert transcriptions to subtitle format\n",
        "def generate_srt(transcription_results, output_srt_path):\n",
        "    with open(output_srt_path, 'w') as f:\n",
        "        index = 1\n",
        "        for segment in transcription_results:\n",
        "            start_time = segment['start']\n",
        "            end_time = segment['end']\n",
        "            text = segment['text']\n",
        "\n",
        "            # Convert start and end times to the format HH:MM:SS,MS\n",
        "            start_time_str = str(timedelta(seconds=start_time))\n",
        "            end_time_str = str(timedelta(seconds=end_time))\n",
        "\n",
        "            # Ensure formatting is consistent\n",
        "            start_time_str = start_time_str[:-3]  # Remove excess microseconds\n",
        "            end_time_str = end_time_str[:-3]\n",
        "\n",
        "            f.write(f\"{index}\\n\")\n",
        "            f.write(f\"{start_time_str} --> {end_time_str}\\n\")\n",
        "            f.write(f\"{text}\\n\\n\")\n",
        "            index += 1\n",
        "\n",
        "# Main pipeline\n",
        "def process_audio_to_subtitles(input_audio_path, output_srt_path):\n",
        "    chunks = split_audio(input_audio_path, chunk_length_ms=30000)  # Split audio into 30s chunks\n",
        "    transcription_results = []\n",
        "\n",
        "    # Process each chunk\n",
        "    for chunk in track(chunks, description=\"Processing 30s chunks one by one\"):\n",
        "        result = transcribe_and_translate(chunk, model)\n",
        "        transcription_results.extend(result['segments'])  # Add each segment from the result\n",
        "\n",
        "    # Generate subtitle file from transcription results\n",
        "    generate_srt(transcription_results, output_srt_path)\n",
        "    print(f\"Subtitle file saved to {output_srt_path}\")"
      ],
      "metadata": {
        "id": "36hhWpbROP4Y"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "from pydub import AudioSegment\n",
        "import os\n",
        "\n",
        "# Initialize Whisper model\n",
        "model = whisper.load_model(\"base\")  # You can use \"small\", \"medium\", or \"large\" depending on your needs\n",
        "\n",
        "# Example usage\n",
        "input_audio_path = \"/content/drive/MyDrive/audio_transcribe/audio_sciml_lecture.m4a\"\n",
        "output_srt_path = \"/content/drive/MyDrive/audio_transcribe/output_subs.srt\"\n",
        "\n",
        "process_audio_to_subtitles(input_audio_path, output_srt_path)"
      ],
      "metadata": {
        "id": "_LDMeA2sIv7U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122,
          "referenced_widgets": [
            "cec95be4d1e944f99beb1205a9eeff1c",
            "ad693ff2524f406d9cc5d026a0b138d6"
          ]
        },
        "outputId": "e09eba65-3e2a-4dcf-8334-8c7e788337ef"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cec95be4d1e944f99beb1205a9eeff1c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subtitle file saved to /content/drive/MyDrive/audio_transcribe/output_subs.srt\n"
          ]
        }
      ]
    }
  ]
}